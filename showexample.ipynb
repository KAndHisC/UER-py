{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaj/miniconda3/envs/uer/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert because of the following error (look up to see its traceback):\nlibssl.so.10: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/uer/lib/python3.8/site-packages/transformers/utils/import_utils.py:1093\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1093\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   1094\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/uer/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:961\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/uer/lib/python3.8/site-packages/transformers/models/__init__.py:19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# module, but to preserve other warnings. So, don't check this module at all.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     albert,\n\u001b[1;32m     21\u001b[0m     audio_spectrogram_transformer,\n\u001b[1;32m     22\u001b[0m     auto,\n\u001b[1;32m     23\u001b[0m     bart,\n\u001b[1;32m     24\u001b[0m     barthez,\n\u001b[1;32m     25\u001b[0m     bartpho,\n\u001b[1;32m     26\u001b[0m     beit,\n\u001b[1;32m     27\u001b[0m     bert,\n\u001b[1;32m     28\u001b[0m     bert_generation,\n\u001b[1;32m     29\u001b[0m     bert_japanese,\n\u001b[1;32m     30\u001b[0m     bertweet,\n\u001b[1;32m     31\u001b[0m     big_bird,\n\u001b[1;32m     32\u001b[0m     bigbird_pegasus,\n\u001b[1;32m     33\u001b[0m     blenderbot,\n\u001b[1;32m     34\u001b[0m     blenderbot_small,\n\u001b[1;32m     35\u001b[0m     bloom,\n\u001b[1;32m     36\u001b[0m     bort,\n\u001b[1;32m     37\u001b[0m     byt5,\n\u001b[1;32m     38\u001b[0m     camembert,\n\u001b[1;32m     39\u001b[0m     canine,\n\u001b[1;32m     40\u001b[0m     chinese_clip,\n\u001b[1;32m     41\u001b[0m     clip,\n\u001b[1;32m     42\u001b[0m     clipseg,\n\u001b[1;32m     43\u001b[0m     codegen,\n\u001b[1;32m     44\u001b[0m     conditional_detr,\n\u001b[1;32m     45\u001b[0m     convbert,\n\u001b[1;32m     46\u001b[0m     convnext,\n\u001b[1;32m     47\u001b[0m     cpm,\n\u001b[1;32m     48\u001b[0m     ctrl,\n\u001b[1;32m     49\u001b[0m     cvt,\n\u001b[1;32m     50\u001b[0m     data2vec,\n\u001b[1;32m     51\u001b[0m     deberta,\n\u001b[1;32m     52\u001b[0m     deberta_v2,\n\u001b[1;32m     53\u001b[0m     decision_transformer,\n\u001b[1;32m     54\u001b[0m     deformable_detr,\n\u001b[1;32m     55\u001b[0m     deit,\n\u001b[1;32m     56\u001b[0m     detr,\n\u001b[1;32m     57\u001b[0m     dialogpt,\n\u001b[1;32m     58\u001b[0m     dinat,\n\u001b[1;32m     59\u001b[0m     distilbert,\n\u001b[1;32m     60\u001b[0m     dit,\n\u001b[1;32m     61\u001b[0m     donut,\n\u001b[1;32m     62\u001b[0m     dpr,\n\u001b[1;32m     63\u001b[0m     dpt,\n\u001b[1;32m     64\u001b[0m     electra,\n\u001b[1;32m     65\u001b[0m     encoder_decoder,\n\u001b[1;32m     66\u001b[0m     ernie,\n\u001b[1;32m     67\u001b[0m     esm,\n\u001b[1;32m     68\u001b[0m     flaubert,\n\u001b[1;32m     69\u001b[0m     flava,\n\u001b[1;32m     70\u001b[0m     fnet,\n\u001b[1;32m     71\u001b[0m     fsmt,\n\u001b[1;32m     72\u001b[0m     funnel,\n\u001b[1;32m     73\u001b[0m     glpn,\n\u001b[1;32m     74\u001b[0m     gpt2,\n\u001b[1;32m     75\u001b[0m     gpt_neo,\n\u001b[1;32m     76\u001b[0m     gpt_neox,\n\u001b[1;32m     77\u001b[0m     gpt_neox_japanese,\n\u001b[1;32m     78\u001b[0m     gptj,\n\u001b[1;32m     79\u001b[0m     groupvit,\n\u001b[1;32m     80\u001b[0m     herbert,\n\u001b[1;32m     81\u001b[0m     hubert,\n\u001b[1;32m     82\u001b[0m     ibert,\n\u001b[1;32m     83\u001b[0m     imagegpt,\n\u001b[1;32m     84\u001b[0m     jukebox,\n\u001b[1;32m     85\u001b[0m     layoutlm,\n\u001b[1;32m     86\u001b[0m     layoutlmv2,\n\u001b[1;32m     87\u001b[0m     layoutlmv3,\n\u001b[1;32m     88\u001b[0m     layoutxlm,\n\u001b[1;32m     89\u001b[0m     led,\n\u001b[1;32m     90\u001b[0m     levit,\n\u001b[1;32m     91\u001b[0m     lilt,\n\u001b[1;32m     92\u001b[0m     longformer,\n\u001b[1;32m     93\u001b[0m     longt5,\n\u001b[1;32m     94\u001b[0m     luke,\n\u001b[1;32m     95\u001b[0m     lxmert,\n\u001b[1;32m     96\u001b[0m     m2m_100,\n\u001b[1;32m     97\u001b[0m     marian,\n\u001b[1;32m     98\u001b[0m     markuplm,\n\u001b[1;32m     99\u001b[0m     maskformer,\n\u001b[1;32m    100\u001b[0m     mbart,\n\u001b[1;32m    101\u001b[0m     mbart50,\n\u001b[1;32m    102\u001b[0m     mctct,\n\u001b[1;32m    103\u001b[0m     megatron_bert,\n\u001b[1;32m    104\u001b[0m     megatron_gpt2,\n\u001b[1;32m    105\u001b[0m     mluke,\n\u001b[1;32m    106\u001b[0m     mmbt,\n\u001b[1;32m    107\u001b[0m     mobilebert,\n\u001b[1;32m    108\u001b[0m     mobilenet_v1,\n\u001b[1;32m    109\u001b[0m     mobilenet_v2,\n\u001b[1;32m    110\u001b[0m     mobilevit,\n\u001b[1;32m    111\u001b[0m     mpnet,\n\u001b[1;32m    112\u001b[0m     mt5,\n\u001b[1;32m    113\u001b[0m     mvp,\n\u001b[1;32m    114\u001b[0m     nat,\n\u001b[1;32m    115\u001b[0m     nezha,\n\u001b[1;32m    116\u001b[0m     nllb,\n\u001b[1;32m    117\u001b[0m     nystromformer,\n\u001b[1;32m    118\u001b[0m     openai,\n\u001b[1;32m    119\u001b[0m     opt,\n\u001b[1;32m    120\u001b[0m     owlvit,\n\u001b[1;32m    121\u001b[0m     pegasus,\n\u001b[1;32m    122\u001b[0m     pegasus_x,\n\u001b[1;32m    123\u001b[0m     perceiver,\n\u001b[1;32m    124\u001b[0m     phobert,\n\u001b[1;32m    125\u001b[0m     plbart,\n\u001b[1;32m    126\u001b[0m     poolformer,\n\u001b[1;32m    127\u001b[0m     prophetnet,\n\u001b[1;32m    128\u001b[0m     qdqbert,\n\u001b[1;32m    129\u001b[0m     rag,\n\u001b[1;32m    130\u001b[0m     realm,\n\u001b[1;32m    131\u001b[0m     reformer,\n\u001b[1;32m    132\u001b[0m     regnet,\n\u001b[1;32m    133\u001b[0m     rembert,\n\u001b[1;32m    134\u001b[0m     resnet,\n\u001b[1;32m    135\u001b[0m     retribert,\n\u001b[1;32m    136\u001b[0m     roberta,\n\u001b[1;32m    137\u001b[0m     roc_bert,\n\u001b[1;32m    138\u001b[0m     roformer,\n\u001b[1;32m    139\u001b[0m     segformer,\n\u001b[1;32m    140\u001b[0m     sew,\n\u001b[1;32m    141\u001b[0m     sew_d,\n\u001b[1;32m    142\u001b[0m     speech_encoder_decoder,\n\u001b[1;32m    143\u001b[0m     speech_to_text,\n\u001b[1;32m    144\u001b[0m     speech_to_text_2,\n\u001b[1;32m    145\u001b[0m     splinter,\n\u001b[1;32m    146\u001b[0m     squeezebert,\n\u001b[1;32m    147\u001b[0m     swin,\n\u001b[1;32m    148\u001b[0m     swinv2,\n\u001b[1;32m    149\u001b[0m     switch_transformers,\n\u001b[1;32m    150\u001b[0m     t5,\n\u001b[1;32m    151\u001b[0m     table_transformer,\n\u001b[1;32m    152\u001b[0m     tapas,\n\u001b[1;32m    153\u001b[0m     tapex,\n\u001b[1;32m    154\u001b[0m     time_series_transformer,\n\u001b[1;32m    155\u001b[0m     trajectory_transformer,\n\u001b[1;32m    156\u001b[0m     transfo_xl,\n\u001b[1;32m    157\u001b[0m     trocr,\n\u001b[1;32m    158\u001b[0m     unispeech,\n\u001b[1;32m    159\u001b[0m     unispeech_sat,\n\u001b[1;32m    160\u001b[0m     van,\n\u001b[1;32m    161\u001b[0m     videomae,\n\u001b[1;32m    162\u001b[0m     vilt,\n\u001b[1;32m    163\u001b[0m     vision_encoder_decoder,\n\u001b[1;32m    164\u001b[0m     vision_text_dual_encoder,\n\u001b[1;32m    165\u001b[0m     visual_bert,\n\u001b[1;32m    166\u001b[0m     vit,\n\u001b[1;32m    167\u001b[0m     vit_mae,\n\u001b[1;32m    168\u001b[0m     vit_msn,\n\u001b[1;32m    169\u001b[0m     wav2vec2,\n\u001b[1;32m    170\u001b[0m     wav2vec2_conformer,\n\u001b[1;32m    171\u001b[0m     wav2vec2_phoneme,\n\u001b[1;32m    172\u001b[0m     wav2vec2_with_lm,\n\u001b[1;32m    173\u001b[0m     wavlm,\n\u001b[1;32m    174\u001b[0m     whisper,\n\u001b[1;32m    175\u001b[0m     x_clip,\n\u001b[1;32m    176\u001b[0m     xglm,\n\u001b[1;32m    177\u001b[0m     xlm,\n\u001b[1;32m    178\u001b[0m     xlm_prophetnet,\n\u001b[1;32m    179\u001b[0m     xlm_roberta,\n\u001b[1;32m    180\u001b[0m     xlm_roberta_xl,\n\u001b[1;32m    181\u001b[0m     xlnet,\n\u001b[1;32m    182\u001b[0m     yolos,\n\u001b[1;32m    183\u001b[0m     yoso,\n\u001b[1;32m    184\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/uer/lib/python3.8/site-packages/transformers/models/mt5/__init__.py:40\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m is_tokenizers_available():\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mt5\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenization_t5_fast\u001b[39;00m \u001b[39mimport\u001b[39;00m T5TokenizerFast\n\u001b[1;32m     41\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/uer/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List, Optional, Tuple\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenization_utils_fast\u001b[39;00m \u001b[39mimport\u001b[39;00m PreTrainedTokenizerFast\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m is_sentencepiece_available, logging\n",
      "File \u001b[0;32m~/miniconda3/envs/uer/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Dict, List, Optional, Tuple, Union\n\u001b[0;32m---> 25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtokenizers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpre_tokenizers\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpre_tokenizers_fast\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtokenizers\u001b[39;00m \u001b[39mimport\u001b[39;00m Encoding \u001b[39mas\u001b[39;00m EncodingFast\n",
      "File \u001b[0;32m~/miniconda3/envs/uer/lib/python3.8/site-packages/tokenizers/__init__.py:79\u001b[0m\n\u001b[1;32m     76\u001b[0m     CONTIGUOUS \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcontiguous\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 79\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtokenizers\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     80\u001b[0m     Tokenizer,\n\u001b[1;32m     81\u001b[0m     Encoding,\n\u001b[1;32m     82\u001b[0m     AddedToken,\n\u001b[1;32m     83\u001b[0m     Regex,\n\u001b[1;32m     84\u001b[0m     NormalizedString,\n\u001b[1;32m     85\u001b[0m     PreTokenizedString,\n\u001b[1;32m     86\u001b[0m     Token,\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     88\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtokenizers\u001b[39;00m \u001b[39mimport\u001b[39;00m decoders\n",
      "\u001b[0;31mImportError\u001b[0m: libssl.so.10: cannot open shared object file: No such file or directory",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n\u001b[1;32m      2\u001b[0m \u001b[39m# tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-lyric\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-lyric\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39m./models/sanwen\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/uer/lib/python3.8/site-packages/transformers/utils/import_utils.py:1083\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[1;32m   1082\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m-> 1083\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[1;32m   1084\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1085\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/uer/lib/python3.8/site-packages/transformers/utils/import_utils.py:1095\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m   1094\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1095\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1096\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1097\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1098\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert because of the following error (look up to see its traceback):\nlibssl.so.10: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-lyric\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-lyric\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/home/jaj/workspace/UER-py/models/sanwen\")\n",
    "model = GPT2LMHeadModel.from_pretrained('/home/jaj/workspace/UER-py/models/sanwen')\n",
    "text_generator = TextGenerationPipeline(model, tokenizer)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '梅花开在南山上， 你 就 可 以 一 睹 百 花 竞 开 、 万 物 争 翠 的 景 象 了 。 那 天 ， 江 西 省 的 百 花 盛 开 正 如 春 光 的 花 朵 。 在 江 西 境 内 的 庐 山 ， 有 条 名 为 东 坡 景 的 街 上 ， 就 连 千 手 观 音 的 塑 像 都 挂 满 了 百 花 。 在 一 百 米 深 处 ， 有 一 棵 十 里 长 的 百 花 观 音 ， 正 在 修 剪 树 枝 ， 正 在 修 剪 枝 头 。 这 是 百 花 繁 放 的 最 好 季 节 上 中 下 秋 。 在 江 西 庐 山 之 间 的 百 花 集 中 地 ， 在 秋 天 ， 各 种 各 样 的 花 丛 树 叶 都 是 金 黄 的 ， 满 眼 的 姹 紫 嫣 红 ， 如 同 一 幅 色 彩 绚 丽 富 丽 的 画 卷 。 而 在 金 色 的 大 千 台 ， 则 是 每 天 会 在 树 枝 、 花 朵 上 绽 放 的 花 朵 。 这 么 多 种 花 ， 都 是 百 花 争 放 的 最 佳 地 方 ， 花 开 花 谢 ， 是 这 个 季 节 最 不 可 错 过 的 花'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\"梅花开在南山上，\", max_length=256, do_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[{'generated_text': '夜空中最亮的星， 这 是 爱 ， 就 此 一 生 ， 愿 我 的 我 的 吻 ， 一 再 爱 ， 也 许 是 我 真 ， 想 对 你 ， 狂 妄 代 价 ， 会 些 为 了 ， 这 种 罪 ， 我 爱 你 ， 没 有 未 能 令 我 的 手 ， 嗯 。 ， 不 要 独 立 生 活 下 也 记 忆 为 我 们 的 心 恼 ， 明 天 可 躲 ， 当 我 的 我 这 单 车 ， 失'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '最美的不是下雨天，是曾与你躲过雨的屋檐 檐 畔 水 ， she 也 同 ， 爱 情 能 ， 距 离 别 人 离 别 海 关 系 ， 我 ， 郎⌒⌒⌒ 算 是 鹜 几 年 ， 就 有 新 天 都 可 以 放 弃 ， 为 何 日 夜 我 ， don don don don don don don 的 路 口 ， 为 感 觉 总 是 一 生 ， 我 的 眼 泪 ， 在 我 的 多 感 情 感'}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\"最美的不是下雨天，是曾与你躲过雨的屋檐\", max_length=100, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '当两颗心开始震动,当你瞳孔学会闪躲 躲 ， 你 去 没 法 术 一 颗 心 的 痕 ， 我 们 欠 浮 现 ， 现 今 我 臂 弯 曲 中 的 意 承 认 过 的 心 中 的 苦 ， 星 的 心 的 歌 啊 ， 我 的 相 展 现 吧 送 ， 每 次 的 脸 上 的 血 性 感 交 给 你 的 心 的 眼 前 的 话 ， 你 不 释 放 在 怀 ， 我 们 都 称'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\"当两颗心开始震动,当你瞳孔学会闪躲\", max_length=100, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '当 两 颗 心 开 始 震 动  ， 快 乐 呀 ， 唔 唔 係 着 猎 猎 猎 猎 点 win7 win7 ， 唔 的 爱 笑 容 ， 结 伴 行 ， 在 春 天 ， 悬 在 你 不 见 途 谈 谈 快 维 维 维 维 维 维 维 维 维 千 里 ， 天 ， 换 节 ， 心 里 漫 天 准 吹 过 份 情 有 我 ， 我 知 结 伴 送 ， 要 淡 ， 想 象 是 在 祝 三 百 天 ， 爱 你 ， 想 毕'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\"当 两 颗 心 开 始 震 动 \", max_length=100, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '当两颗心开始震动， 快 乐 呀 ， 误 解 呀 ， 随 着 时 间 都 会 增 长 ， 退 潮 的 爱 像 刀 疤 ， 伤 过 给 一 个 说 法 ， 放 了 才 能 够 快 乐 ， 让 心 好 好 休 息 一 下 ， 握 不 住 的 他 ， 放 下 也 罢 ， 你 给 的 说 法 ， 说 走 到 分 岔 ， 又 无 力 ， 又 疲 倦 ， 付 出 爱 的 代 价 ， 无 力 自'}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\"当两颗心开始震动，\", max_length=100, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-lyric\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-lyric\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/localdata/workspace/UER-py/models/clyric\")\n",
    "model = GPT2LMHeadModel.from_pretrained('/localdata/workspace/UER-py/models/clyric')\n",
    "text_generator = TextGenerationPipeline(model, tokenizer)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [{'generated_text': '最美的不是下雨天，是曾与你躲过雨的屋檐 ， 下 课 铃 声 响 起 的 瞬 间 ， 我 们 的 笑 脸 ， 有 太 多 回 忆 在 浮 现 ， 是 你 总 在 我 身 边 ， 不 知 道 会 不 会 再 见 ， 从 现 在 开 始 到 永 远 ， 想 说 的 语 言 凝 结 成 一 句 ， 不 管 我 们 是 否 能 够 兑 现 ， 想 说 的 语 言 凝 结'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc929b8184b97e264f70f5284f280b8cccade275b07b2944f5e202851ce07696"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
