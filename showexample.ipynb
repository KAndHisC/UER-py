{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-lyric\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-lyric\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/home/wangshichuan/workspace/potery/UER-py/models/clue\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"/home/wangshichuan/workspace/potery/UER-py/models/clue\")\n",
    "\n",
    "text_generator = TextGenerationPipeline(model, tokenizer)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '一 千 朵 雪 落 无 声 ↲ 我 把 日 子 铸 成 它 ↲ 填 进 泥 土 ↲ 填 满 一 个 < 自 己 的 日 子 ↲ 像 一 只 收 集 病 菌 的 老 鼠 ↲ 播 种 革 命 的 火 种 ， 掉 弄 灵 巧 的 概 念 ↲ 将 王 宫 搞 得 惴 惴 不 安 ↲ 他 兴 奋 他 战 栗 他 表 皮 敏 感 ↲ 自 恋 得 发 狂 自 画 像 就 画 了 四 五 张 ↲ 得 意 的 表 达 ↲ 他 在 摇 撼 他 想 写 的 就 是 想 象 ↲ 西 的 那 条 河 上 的 天 边 ↲ 着 他 ↲ ， 他 的 时 间 ↲ 他 的 花 园 已 是 他 的 脸 像 他 的 天 气 ， 仿 佛 是 一 天 的 ↲ ， 他 的 天 上 ， 他 的 向 他 的 天 ↲ 但 的 ， 不 是 一 生 活 的 是 不 得 的 人 的 ↲ 他 的 他 的 床 边 上 ， 他 ， 他 的 轮 ↲ 不 滚 滚 动 的 水 的 嘴 ， 逐 渐 渐 渐 渐 渐 渐 渐 渐 渐 明 天 上 的 ↲ 他 的 变 成 为 ↲ 不 是 一 生 命 运 到 他 的 说 ， 向 上 ， 像 中 的 ， 向 ↲ ，'},\n",
       " {'generated_text': '一 千 朵 雪 落 无 声 ↲ 我 把 日 子 铸 成 它 ↲ 填 进 泥 土 ↲↲ 我 不 再 是 水 ， 是 树 ↲ 在 日 蚀 的 草 丛 中 ↲ 我 不 在 乎 它 们 是 我 ↲ 在 我 生 日 的 时 候 遇 见 它 ↲ 通 而 来 的 一 种 无 声 的 表 情 ↲ 着 夜 空 ↲ 我 在 一 瞬 间 ↲↲ 这 是 我 的 生 命 ↲ 我 开 始 平 静 而 舒 缓 ↲ 这 是 我 生 命 中 的 生 命 ↲ 我 不 满 意 所 有 过 路 人 ↲ 着 他 们 的 偶 像 ↲ ↲ 于 我 的 笑 ↲↲ 我 的 树 ↲ 我 的 我 的 宽 阔 遇 到 来 感 觉 察 看 我 的 ↲ 我 的 面 前 行 走 了 我 的 舞 足 ↲ 我 的 错 误 我 的 我 ↲ 我 的 眼 睛 ↲↲ 我 的 悲 哀 伤 害 我 ↲ 使 我 的 楚 ↲ 痛 楚 ↲ 他 们 ， 他 们 ↲ 我 的 悲 哀 伤 害 了 ↲ 我 的 我 ↲ 我 ↲ 我 的 心 事 ↲ 我 的 雨 、 我 ↲ 我 的 玻 璃 ↲ 我 的 ↲ 我 自 行 列 复 杂 的 ↲ 我 ↲ 我 ↲↲ 我 ↲↲ 我'},\n",
       " {'generated_text': '一 千 朵 雪 落 无 声 ↲ 我 把 日 子 铸 成 它 ↲ 填 进 泥 土 ↲ 填 满 一 个 < 亲 近 小 屋 ↲↲ 废 的 日 子 ↲ 像 一 堆 无 知 的 树 ↲ 浓 雾 裹 住 我 的 门 ↲↲ 我 的 四 周 ↲ 被 春 风 遗 忘 着 ↲ 出 我 的 双 手 ↲↲ 伤 口 ， 偷 偷 ↲ 我 的 声 音 被 你 伤 痛 ↲ 我 被 你 的 带 领 ↲ 震 撼 浓 云 ↲↲ 我 不 能 痛 饮 ↲ 那 不 能 抵 抗 的 ↲ ↲ 的 ↲↲ 尖 的 雪 ↲ 的 ↲ ↲ 如 ↲ ↲ 满 床 ↲ 星 ↲ 如 ↲ 一 个 ↲ 星 星 ↲ 然 ↲ ↲ ↲ ↲ 我 ↲ 的 ↲ 星 ↲ 不 醒 ↲ ↲ 说 ↲ 奔 跑 ↲ 掠 过 ↲ 我 ↲ ↲ 我 的 ↲ ↲ 星 ↲ ↲ ↲ 星 星 ↲ 我 ↲ ↲ ↲ ↲ 掠 ↲ 我 的 不 能 ↲ 我 ↲ 古 劫 持 续 的 ↲ 我 ↲ ↲ 星 ↲ 星 星 星 星 ↲ 我 的 ↲ 我 ↲ ↲ 我 ↲ 我 的 ↲ ↲ 我 ↲ 星 ↲ ↲ 星 星 星'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\"一 千 朵 雪 落 无 声 ↲ 我 把 日 子 铸 成 它 ↲ 填 进 泥 土 ↲\", max_length=256, do_sample=True,top_k=5, top_p=0.95, num_return_sequences=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[{'generated_text': '梅花落满了南山 ↲ 啊 ， 多 少 人 看 多 了 江 南 阴 冷 ↲↲ 这 个 月 我 多 么 渴 望 能 够 ↲ 当 我 看 到 你 开 花 了 ↲ 你 的 日 子 正 变 成 一 条 金 色 的 小 河 ↲ 这 是 我 对 你 无 言 的 流 向 ↲ 西 的 这 块 麦 田 ↲↲ 这 是 你 的 灵 魂 ↲ 你 手 里 紧 握 着 麦 穗 ↲ 你 含 泪 的 彩 色 ↲ 肯 定 留 下 我 在 这 里 ↲ 入 我 的 眼 睛 ↲↲ 尖 的 树 林 ↲ 一 切 ↲ 我 ↲ ↲ 我 的 持 你 ， 4 笑 ↲ 我 ↲ 我 ↲ 我 ↲ 裂 ↲↲ 我 的 ↲ 古 树 冠 冕 ↲ 我 ↲ 我 的 热 ↲ 我 ↲ 我 的 小 话 ↲ 我 的 ↲ ↲ 我 的 ↲ ↲ 我 的 窗 ↲ 鱼 宫 殿 堂 的 人 的 小 事 ↲ 我 ↲ 我 站 在 泥 土 摇 晃 动 ， 我 ↲ 一 道 路 上 ↲ 我 ↲ 我 的 人 ↲ 我 ↲ 我 的 小 宫 殿 堂 ↲ ↲ 我 ， ↲ 我 ↲ 我 的 心 事 ↲↲ 叶 荫 的 心 ↲ 的 ↲ 我'},\n",
    " {'generated_text': '梅花落满了南山 ↲ 一 千 年 的 寒 风 ↲↲ 七 月 ， 那 人 来 得 满 园 子 的 一 生 ↲ 定 在 谷 外 拋 锚 的 花 园 围 栏 ↲↲ 我 看 见 你 长 大 了 饥 饿 的 名 字 ↲ 一 下 ， 你 的 名 字 在 山 谷 ↲ 仿 佛 一 粒 黏 星 ↲↲ 我 穿 著 绿 洲 的 草 坡 ↲ 又 一 坡 的 寂 寞 ↲ 金 黄 的 手 指 插 满 了 风 ↲↲ 我 的 心 情 ↲ 我 要 贴 着 白 沙 的 青 苔 ↲ 贴 到 蓝 色 的 胸 膛 ↲ ↲ ↲ 绒 上 ↲ 我 的 床 ↲ 我 的 心 灵 魂 魄 ↲ 上 游 荡 的 柔 和 你 的 天 空 气 息 虚 伪 装 饰 ↲↲ 午 睡 梦 的 浊 的 心 ↲ 我 的 心 ↲ 我 的 床 边 ↲ 络 时 候 鸟 ↲ 的 心 ↲ 的 日 子 里 ， ↲ 让 我 的 日 子 的 心 的 日 子 上 ↲↲ 我 ↲↲ 我 的 ↲↲ 的 早 晨 曦 ↲ 黄 昏 暗 绿 ↲↲ 尖 锐 利 的 阳 光 焰 ↲ 面 ↲ 全 部 分 ↲ 让 我 日 子 的 心 里 ，'},\n",
    " {'generated_text': '梅花落满了南山 ↲↲ 冬 日 的 雨 下 在 江 湖 ↲ 江 湖 注 定 是 你 诗 中 的 一 个 险 句 ↲↲ 不 如 学 仙 去 ↲ 无 端 端 的 你 ↲ 不 能 不 去 ↲↲ 你 是 我 的 好 友 ↲ 你 是 我 的 朋 友 ↲ 我 们 是 你 的 兄 弟 ↲ 你 是 我 的 朋 友 ↲ 永 远 在 你 的 心 里 ↲ 你 是 一 首 诗 ↲↲ 你 是 一 朵 看 到 的 ↲ 你 的 眼 睛 ↲ 永 不 用 的 思 想 ↲ 永 不 长 的 ↲↲ 你 来 了 ↲ ↲ ↲ 的 ↲ ↲ ↲ 你 的 ↲ 哪 一 巴 ↲ 一 滴 ↲ 哪 一 个 ↲ ↲ 我 的 ↲ ↲ 我 ↲ 船 ↲ ↲ ↲ 我 ↲ 你 走 ↲ 到 你 的 ↲ 你 走 了 ↲ 你 ↲ 我 的 ↲ 的 ↲ 你 的 ↲ 我 的 你 的 ↲ ↲ ↲ ↲ ↲ ↲ ↲ 的 ↲ 你 的 ↲ ↲ 掠 价 ↲ 掠 过 ↲ 你 的 ↲ 你 掠 过 ↲ ↲ 我 的 ↲ ↲ 我 ↲ ↲ ↲ ↲ ↲ 你 的 ↲ 于 忧 愁 ↲'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[{'generated_text': '夜空中最亮的星， 这 是 爱 ， 就 此 一 生 ， 愿 我 的 我 的 吻 ， 一 再 爱 ， 也 许 是 我 真 ， 想 对 你 ， 狂 妄 代 价 ， 会 些 为 了 ， 这 种 罪 ， 我 爱 你 ， 没 有 未 能 令 我 的 手 ， 嗯 。 ， 不 要 独 立 生 活 下 也 记 忆 为 我 们 的 心 恼 ， 明 天 可 躲 ， 当 我 的 我 这 单 车 ， 失'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '{冬}<3>[梅花,雪,寒冷,思念]梅花落满了南山，像一地的雪， ， ， ， 一 样 。 ， 的 海 滩 的 小 草 滩 ， ， ， 还 是 得 很 早 年 的 ， 在 海 洋 的 的 人 类 似 的 时 间 的 尽 头 ， 。 ， ， 的 得 很 远 方 ， 我 的 的 一 个 时 候 。 ， 地 上'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\"\\{冬\\}<>[梅花,雪,寒冷,思念]梅花落满了南山，像一地的雪，\", max_length=128, do_sample=True,top_k=5, top_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator(\"最美的不是下雨天，是曾与你躲过雨的屋檐\", max_length=256, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator(\"当两颗心开始震动,当你瞳孔学会闪躲\", max_length=100, do_sample=True)\n",
    "text_generator(\"当两颗心开始震动,当你瞳孔学会闪躲\", max_length=100, do_sample=True,     top_k=5, top_p=0.9, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator(\"当 两 颗 心 开 始 震 动 \", max_length=100, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator(\"当两颗心开始震动，\", max_length=100, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-lyric\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-lyric\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/localdata/workspace/UER-py/models/clyric\")\n",
    "model = GPT2LMHeadModel.from_pretrained('/localdata/workspace/UER-py/models/clyric')\n",
    "text_generator = TextGenerationPipeline(model, tokenizer)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [{'generated_text': '最美的不是下雨天，是曾与你躲过雨的屋檐 ， 下 课 铃 声 响 起 的 瞬 间 ， 我 们 的 笑 脸 ， 有 太 多 回 忆 在 浮 现 ， 是 你 总 在 我 身 边 ， 不 知 道 会 不 会 再 见 ， 从 现 在 开 始 到 永 远 ， 想 说 的 语 言 凝 结 成 一 句 ， 不 管 我 们 是 否 能 够 兑 现 ， 想 说 的 语 言 凝 结'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaj/miniconda3/envs/uer/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 713k/713k [00:01<00:00, 676kB/s]  \n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nTFGPT2LMHeadModel requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[39mreturn\u001b[39;00m text\n\u001b[1;32m     16\u001b[0m tokenizer \u001b[39m=\u001b[39m XLNetTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mmymusise/CPM-Generate-distill\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m model \u001b[39m=\u001b[39m TFGPT2LMHeadModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmymusise/CPM-Generate-distill\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     19\u001b[0m text_generater \u001b[39m=\u001b[39m TextGenerationPipeline(model, tokenizer)\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(text_generater(\u001b[39m\"\u001b[39m\u001b[39m天下熙熙，\u001b[39m\u001b[39m\"\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, top_k\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, use_cache\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/uer/lib/python3.8/site-packages/transformers-4.6.1-py3.8.egg/transformers/utils/dummy_tf_objects.py:955\u001b[0m, in \u001b[0;36mTFGPT2LMHeadModel.from_pretrained\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    954\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 955\u001b[0m     requires_backends(\u001b[39mself\u001b[39;49m, [\u001b[39m\"\u001b[39;49m\u001b[39mtf\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/miniconda3/envs/uer/lib/python3.8/site-packages/transformers-4.6.1-py3.8.egg/transformers/file_utils.py:569\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m    567\u001b[0m name \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(BACKENDS_MAPPING[backend][\u001b[39m0\u001b[39m]() \u001b[39mfor\u001b[39;00m backend \u001b[39min\u001b[39;00m backends):\n\u001b[0;32m--> 569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([BACKENDS_MAPPING[backend][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m backend \u001b[39min\u001b[39;00m backends]))\n",
      "\u001b[0;31mImportError\u001b[0m: \nTFGPT2LMHeadModel requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer, GPT2LMHeadModel\n",
    "from transformers import TextGenerationPipeline\n",
    "import jieba\n",
    "# add spicel process \n",
    "class XLNetTokenizer(XLNetTokenizer):\n",
    "    translator = str.maketrans(\" \\n\", \"\\u2582\\u2583\")\n",
    "    def _tokenize(self, text, *args, **kwargs):\n",
    "        text = [x.translate(self.translator) for x in jieba.cut(text, cut_all=False)]\n",
    "        text = \" \".join(text)\n",
    "        return super()._tokenize(text, *args, **kwargs)\n",
    "    def _decode(self, *args, **kwargs):\n",
    "        text = super()._decode(*args, **kwargs)\n",
    "        text = text.replace(' ', '').replace('\\u2582', ' ').replace('\\u2583', '\\n')\n",
    "        return text\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained('mymusise/CPM-Generate-distill')\n",
    "model = GPT2LMHeadModel.from_pretrained(\"mymusise/CPM-Generate-distill\")\n",
    "\n",
    "text_generater = TextGenerationPipeline(model, tokenizer)\n",
    "\n",
    "print(text_generater(\"天下熙熙，\", max_length=15, top_k=1, use_cache=True, prefix=''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator('梅花开在南山上，', max_length=50, do_sample=True, top_p=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49a4d35495a0ee52f35720c27c94986b4a3077a71a8c6157808376aed5c51fad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
